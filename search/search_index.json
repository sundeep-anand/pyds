{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to PyDS For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-pyds","text":"For full documentation visit mkdocs.org .","title":"Welcome to PyDS"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"examples/","text":"Examples CH 1: Introduction Problem-63: Functions by decreasing order of growth. Function Rate of Growth (n + 1)! O(n!) n! O(n!) 4 n O(4 n ) n * 3 n O(n3 n ) 3 n + n 2 + 20n O(3 n ) ( 3 / 2 ) n O(( 3 / 2 ) n ) 4n 2 O(n 2 ) 4 lgn O(n 2 ) n 2 + 200 O(n 2 ) 20n + 500 O(n) 2 lgn O(n) n 2/3 O(n 2/3 ) 1 O(1)","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#ch-1-introduction","text":"","title":"CH 1: Introduction"},{"location":"examples/#problem-63-functions-by-decreasing-order-of-growth","text":"Function Rate of Growth (n + 1)! O(n!) n! O(n!) 4 n O(4 n ) n * 3 n O(n3 n ) 3 n + n 2 + 20n O(3 n ) ( 3 / 2 ) n O(( 3 / 2 ) n ) 4n 2 O(n 2 ) 4 lgn O(n 2 ) n 2 + 200 O(n 2 ) 20n + 500 O(n) 2 lgn O(n) n 2/3 O(n 2/3 ) 1 O(1)","title":"Problem-63: Functions by decreasing order of growth."},{"location":"log/","text":"Log Subject Ch Point When? Misc Other start 0: OoCh 0.1 20190603 - - complete 0: OoCh - 20190624 - - complete 1: Intro - 20190904 - - complete 2: Recur - 20191001 - - niddle 2: Recur 7 20191001 - Problem","title":"Log"},{"location":"log/#log","text":"Subject Ch Point When? Misc Other start 0: OoCh 0.1 20190603 - - complete 0: OoCh - 20190624 - - complete 1: Intro - 20190904 - - complete 2: Recur - 20191001 - - niddle 2: Recur 7 20191001 - Problem","title":"Log"},{"location":"notes/","text":"Notes CH 2: Recursion and Backtracking 2.2 2.3 What is recursion? and Why? A recursive method solves a problem by calling a copy of itself to work on a smaller problem. It is important to ensure that the recursion terminates. The sequence of smaller problems must eventually converge on the base case. Generally, loops are turned into recursive functions when they are compiled or interpreted. 2.5 Recursion and Memory Each recursive call makes a new copy of that method (actually only the variables) in memory. Once a method ends (that is, returns some data), the copy of that returning method is removed from memory. 2.6 Recursion versus Iteration Recursion Terminates when base case is reached. Each recursive call requires extra space on the stack frame (memory). If we get infinite recursion, the program may run out of memory and result in stack overflow. Solutions to some problems are easier to formulate recursively. Examples: Fibonacci Series, Factorial Finding, Merge Sort, Quick Sort, Binary Search, Tree Traversals, Graph Traversals, Dynamic Programming, Divide and Conquer, Towers of Hanoi, and Backtracking Algorithms Iteration Terminates when a condition proven to be false. Each iteration does not require an extra space. An infinite loop could loop forever since there is no extra memory being created. Iterative solutions to a problem may not always be as obvious as a recursive solution. 2.10 What is Backtracking? Backtracking systematically searches for a solution to a problem among all available options. We start with one possible option out of many available options and try to solve the problem if we are able to solve the problem with the selected move then we will print the solution else we will backtrack and select some other option and try to solve it. Backtracking can be thought of as a selective tree/graph traversal method. Sometimes the best algorithm for a problem is to try all possibilities. This is always slow, but there are standard tools that can be used to help. Tools: algorithms for generating basic objects, such as binary strings [2 n possibilities for n-bit string], permutations [n!] , combinations [n!/r!(n-1)!] , general strings [k -ary strings of length n has k n possibilities], etc... Backtracking speeds the exhaustive search by pruning. Example Algorithms: Binary Strings: generating all binary strings Generating k -ary Strings N-Queens Problem The Knapsack Problem Generalized Strings Hamiltonian Cycles Graph Coloring Problem CH 1: Introduction find the complexity of any given algorithm 1.1 Variables Variables are names (x and y) of the placeholders for representing data. 1.2 Data Types A set of data with predefined values, example: integer, floating point, unit number, character, string, etc. In memory we combine 2 bytes (16 bits) and call it an integer. Similarly, 4 bytes (32 bits) for float. System defined data types (Primitive data types) int , float , char , double , bool , etc. User defined data types structures , union , classes 1.3 Data Structures A special format for organizing and storing data so that it can be used efficiently. Example arrays , files , linked list , stacks , queues , trees , graphs and so on. Linear data structure: Linked List, Stacks and Queues. Non-linear data structure: Trees and Graphs. 1.4 Abstract Data Types (ADTs) When we combine the data structures with their operations. An ADT has two parts: Declaration of data Declaration of operations. Few examples are Linked Lists , Stacks , Queues , Priority Queues , Binary Trees , Dictionaries , Disjoint Sets (Union and Find) , Hash Tables , Graphs and many others. 1.5 What is an Algorithm? 1.6 Why the Analysis of Algorithms? The step-by-step unambiguous instructions to solve a given problem. Criteria: Correctness ( Does the algorithm give solution to the problem in a finite number of steps? ) Efficiency ( How much resources - memory and time - does it take to execute? ) Helps us to determine which algorithm is most efficient in terms of time and space consumed. 1.8 What is Running Time Analysis? The process of determining how processing time increases as the size of the problem (input size) increases. Common types: Size of an array Polynomial degree Number of elements in a matrix Number of bits in the binary representation of the input Vertices and edges in a graph 1.9 How to compare Algorithms Let us assume that we express the running time of a given algorithm as a function of the input size n ( i.e. f(n) ) and compare these different functions corresponding to running times. This kind of comparison is independent of machine time, programming style, etc. 1.10 What is rate of growth? The rate at which the running time increases as a function of input is called rate of growth . n 4 + 2n 2 + 100n + 500 ~ n 4 1.11 Commonly Used Rates of Growth Time Complexity Name Example 1 Constant Adding an element to the front of the linked list log n Logarithmic Finding an element in a sorted array n Linear Finding an element in a unsorted array n log n Linear Logarithmic Sorting n items by divide-n-conquer - MergeSort n 2 Quadratic Shortest path between two nodes in a graph n 3 Cubic Matrix Multiplication 2 n Exponential The Towers of Hanoi problem 1.12 Types of Analysis Algorithm can be represented with multiple expressions: one for the case where it takes less time ( best case ) and another for the case where it takes more time ( worst case ). Worst case: Defines the input for which the algorithm takes a long time (slowest time to complete). f(n) = n 2 + 500 Best case: Defines the input for which the algorithm takes the least time (fastest time to complete). f(n) = n + 100n + 500 Average case: Run the algorithm many times, using many different inputs that come from some distribution that generates these inputs, compute the total running time (by adding the individual times), and divide by the number of trails. Lower Bound <= Average Time <= Upper Bound 1.13 Big-O Notation Represented as f(n) = O(g(n)) , where g(n) gives the maximum rate of growth for f(n) at larger values of n. n 0 , called as threshold, is the point from which we need to consider the rate of growth for a given algorithm. Defined as O(g(n)) = {f(n): there exist positive constants c and n 0 such that 0 <= f(n) <= cg(n) for all n >= n 0 } 1.15 Omega-\u03a9 Notation This notation gives the tighter lower bound of the given algorithm and we represent it as f(n) = \u03a9(g(n)) . That means, at larger values of n, the tighter lower bound of f(n) is g(n). 2n = \u03a9(n), n 3 = \u03a9(n 3 ), logn = \u03a9(logn) Defined as \u03a9(g(n)) = {f(n): there exist positive constants c and n 0 such that 0 <= cg(n) <= f(n) for all n >= n 0 } 1.16 Theta-\u0398 Notation This notation decides whether the upper and lower bounds of a given function (algorithm) are the same. If the upper bound (O) and the lower bound (\u03a9) give the same result, then the \u0398 notation will also have the same rate of growth. Defined as \u0398(g(n)) = {f(n): there exist positive constants c 1 , c 2 and n 0 such that 0 <= c 1 g(n) <= f(n) <= c 2 g(n) for all n >= n 0 } For analysis ( best case , worst case and average ), we try to give the upper bound (O) and lower bound (\u03a9) and average running time (\u0398). We generally focus on upper bound (O). g(n) is the asymptotic curve for f(n). 1.18 Guidelines for Asymptotic Analysis Loops : The running time of a loop is the running time of the statements inside the loop (including tests) multiplied by the number of iterations. Total time = a constant c x n = cn = O(n) Nested Loops : Total running time is the product of the sizes of all the loops. Total time = c x n x n = cn 2 = O(n 2 ) Consecutive statements : Add the time complexities of each statement. Total time = c 0 + c 1 n + c 2 n 2 = O(n 2 ) If-then-else statements : Worst-case running time: the test, plus either the then part or the else part. Total time = c 0 + c 1 * n = O(n) Logarithmic complexity : An algorithm is O(logn) if it takes a constant time to cut the problem size by a fraction (usually by 1 / 2 ). Total time = log(2 k ) = klog2 = O(logn) An example: binary search (finding a word in a dictionary of n pages) ( 1 ) Look at the center point in the dictionary ( 2 ) Is the word towards the left or right of the center? ( 3 ) Repeat the process with the left or right part of the dictionary until the word is found. 1.21 Master Theorem for Divide and Conquer Recurrences Divide and conquer algorithms divide the problem into sub-problems, each of which is a part of the original problem, and then perform some additional work to compute the final answer. Example: for a merge sort algorithm, running time equation could be T(n) = 2T( n / 2 ) + O(n) 1.26 Amortized Analysis Amortized analysis is a worst-case analysis, but for a sequence of operations rather than for individual operations. It generally applies to a method that consists of a sequence of operations, where the vast majority of the operations are cheap, but some of the operations are expensive. When one event in a sequence affects the cost of later events: One particular task may be expensive. But it may leave data structure in a state that the next few operations become easier. Example : Let us consider an array of elements from which we want to find the k th smallest element. We can solve this problem using sorting. After sorting the given array, we just need to return the k th element from it. The cost of performing the sort (assuming comparison based sorting algorithm) is O(nlogn). If we perform n such selections then the average cost of each selection is O(nlogn/n) = O(logn). This clearly indicates that sorting once is reducing the complexity of subsequent operations. Observations : While the recurrence relation looks exponential, the solution to the recurrence relation here gives a different result. CH 0: Organization of Chapters Three things required to build good understanding of data structure are: How the information is arranged in the memory of the computer? Become familiar with the algorithms for manipulating the information contained in the data structure. Understand the performance characteristics of the data structure. to select a suitable data structure for a particular application. Definitions Recursion is the process of defining a function or calculating a number by the repeated application of an algorithm. Backtracking The solution process consists of working through a sequence of decision points in which each choice leads further along some path (example Trees or Graphs). In case of incorrect choice somewhere along the way, developer has to backtrack to a previous decision point and try a different path. Backtracking algorithms use this approach, moreover, it's a form of recursion. Linked Lists (dynamic) Number of nodes can grow and shrink on demand. Used to create trees, graphs, hashing, etc. Stacks (abstract) A container of objects that are inserted and removed according to the last-in-first-out LIFO principle. Applications: Internal creation of function parameters and local variables. Compiler's syntax check for matching braces; Support for recursion. Can act as an auxiliary data structure for other abstract data types. Queues (abstract) Similar to stacks but follow FIFO . Applications: In OS, for controlling access to shared system resources such as printers, files, communication lines, disks and tapes. Computer systems must often provide a holding area buffer for messages between two processes, two programs, or even two systems. Can act as an auxiliary data structure for other abstract data types. Trees (abstract) Data organization to make the data insertion or deletion or search faster. Applications: Library db, School's student db, employ or patient db; any db would be implemented using trees. OS file systems; Can act as an auxiliary data structure for other abstract data types. Tree variants are classified by the number of children and the way of interconnecting them. Priority Queues (abstract) Maintain a collection of prioritized elements. Application: operating systems often use a priority queue for the ready queue of processes to run on the CPU. Graphs (fundamental - non linear) Collection of nodes vertices and their connections edges . Used to model many types of relations and processes. Disjoint Set (abstract) Collection of disjoint sets partition . Example: Motorola , YouTube , and Android are all members of the Google set. Two operations: Union: merging of two sets into one! Find Query: takes an item and returns which set it belongs to. Application: Maze generation and Kruskal's algo for computing the minimum spanning tree of a graph. Sorting Arranges the elements of a list in a certain order; sometimes reduces the complexity of problem significantly. Applications: searching elements and db algorithms. Searching The process of finding an item with specified properties from a collection of items. Data organization improves the searching process. Selection Algo ( k th order statistic) Finding the minimum, maximum and median elements in a given list. Symbol Tables (dictionaries) Examples: spelling checker, data dictionary found in DBMS, symbol tables generated by loaders, assemblers, and compilers, as well as routing tables in networking components (DNS Lookup). Hashing Technique used for storing and retrieving information as fast as possible. Used in optimal search and symbol tables. The worst case complexity of hashing is O(n), but it gives O(1) on the average. String Algo Operations like auto completion (on CLI or browser) or string matching need a data structure which stores the string data efficiently. Algo Design Techniques There are different ways of classifying the algorithms like Greedy, Divide and Conquer, and Dynamic Programming. Greedy Algo ( single-minded algorithm) Process that looks for a simple, easy to implement solutions to complex, multi-step problems by deciding which next step will provide the most obvious benefit. Example: selection sort, Prim's algorithms, Kruskal's algorithms, Dijkstra algorithm, Huffman coding algorithm etc. Divide and Conquer Principles: (examples include binary search, merge sort, etc.) Divide - break the problem into several subproblems that are similar to the original problem but smaller in size. Conquer - solve the subproblems recursively. Base case - If the subproblem size is small enough then solve the subproblem directly without more recursion. Combine - the solutions to create a solution for the original problem. Dynamic Programming (optimization over plain recursion) Breaking down a complex problem into a collection of simpler subproblems. Solving each of those and storing their solutions using a memory-based data structure (array, map, etc). Complexity Classes (resources required during computation - time vs space) Classification is done based on the running time (or memory) that an algorithm takes for solving the problem. Bit-wise Hacking Applications: Kind of file format or network protocol that uses individual bits or group of bits to represent pieces of information. Setting individual pixels on the screen by directly manipulating the video memory, where every pixel's color is represented by 1 or 4 bits. Compute some kind of checksum (possibly, parity or CRC) or hash value.","title":"Notes"},{"location":"notes/#notes","text":"","title":"Notes"},{"location":"notes/#ch-2-recursion-and-backtracking","text":"","title":"CH 2: Recursion and Backtracking"},{"location":"notes/#22-23-what-is-recursion-and-why","text":"A recursive method solves a problem by calling a copy of itself to work on a smaller problem. It is important to ensure that the recursion terminates. The sequence of smaller problems must eventually converge on the base case. Generally, loops are turned into recursive functions when they are compiled or interpreted.","title":"2.2 2.3 What is recursion? and Why?"},{"location":"notes/#25-recursion-and-memory","text":"Each recursive call makes a new copy of that method (actually only the variables) in memory. Once a method ends (that is, returns some data), the copy of that returning method is removed from memory.","title":"2.5 Recursion and Memory"},{"location":"notes/#26-recursion-versus-iteration","text":"Recursion Terminates when base case is reached. Each recursive call requires extra space on the stack frame (memory). If we get infinite recursion, the program may run out of memory and result in stack overflow. Solutions to some problems are easier to formulate recursively. Examples: Fibonacci Series, Factorial Finding, Merge Sort, Quick Sort, Binary Search, Tree Traversals, Graph Traversals, Dynamic Programming, Divide and Conquer, Towers of Hanoi, and Backtracking Algorithms Iteration Terminates when a condition proven to be false. Each iteration does not require an extra space. An infinite loop could loop forever since there is no extra memory being created. Iterative solutions to a problem may not always be as obvious as a recursive solution.","title":"2.6 Recursion versus Iteration"},{"location":"notes/#210-what-is-backtracking","text":"Backtracking systematically searches for a solution to a problem among all available options. We start with one possible option out of many available options and try to solve the problem if we are able to solve the problem with the selected move then we will print the solution else we will backtrack and select some other option and try to solve it. Backtracking can be thought of as a selective tree/graph traversal method. Sometimes the best algorithm for a problem is to try all possibilities. This is always slow, but there are standard tools that can be used to help. Tools: algorithms for generating basic objects, such as binary strings [2 n possibilities for n-bit string], permutations [n!] , combinations [n!/r!(n-1)!] , general strings [k -ary strings of length n has k n possibilities], etc... Backtracking speeds the exhaustive search by pruning. Example Algorithms: Binary Strings: generating all binary strings Generating k -ary Strings N-Queens Problem The Knapsack Problem Generalized Strings Hamiltonian Cycles Graph Coloring Problem","title":"2.10 What is Backtracking?"},{"location":"notes/#ch-1-introduction","text":"find the complexity of any given algorithm","title":"CH 1: Introduction"},{"location":"notes/#11-variables","text":"Variables are names (x and y) of the placeholders for representing data.","title":"1.1 Variables"},{"location":"notes/#12-data-types","text":"A set of data with predefined values, example: integer, floating point, unit number, character, string, etc. In memory we combine 2 bytes (16 bits) and call it an integer. Similarly, 4 bytes (32 bits) for float. System defined data types (Primitive data types) int , float , char , double , bool , etc. User defined data types structures , union , classes","title":"1.2 Data Types"},{"location":"notes/#13-data-structures","text":"A special format for organizing and storing data so that it can be used efficiently. Example arrays , files , linked list , stacks , queues , trees , graphs and so on. Linear data structure: Linked List, Stacks and Queues. Non-linear data structure: Trees and Graphs.","title":"1.3 Data Structures"},{"location":"notes/#14-abstract-data-types-adts","text":"When we combine the data structures with their operations. An ADT has two parts: Declaration of data Declaration of operations. Few examples are Linked Lists , Stacks , Queues , Priority Queues , Binary Trees , Dictionaries , Disjoint Sets (Union and Find) , Hash Tables , Graphs and many others.","title":"1.4 Abstract Data Types (ADTs)"},{"location":"notes/#15-what-is-an-algorithm-16-why-the-analysis-of-algorithms","text":"The step-by-step unambiguous instructions to solve a given problem. Criteria: Correctness ( Does the algorithm give solution to the problem in a finite number of steps? ) Efficiency ( How much resources - memory and time - does it take to execute? ) Helps us to determine which algorithm is most efficient in terms of time and space consumed.","title":"1.5 What is an Algorithm? 1.6 Why the Analysis of Algorithms?"},{"location":"notes/#18-what-is-running-time-analysis","text":"The process of determining how processing time increases as the size of the problem (input size) increases. Common types: Size of an array Polynomial degree Number of elements in a matrix Number of bits in the binary representation of the input Vertices and edges in a graph","title":"1.8 What is Running Time Analysis?"},{"location":"notes/#19-how-to-compare-algorithms","text":"Let us assume that we express the running time of a given algorithm as a function of the input size n ( i.e. f(n) ) and compare these different functions corresponding to running times. This kind of comparison is independent of machine time, programming style, etc.","title":"1.9 How to compare Algorithms"},{"location":"notes/#110-what-is-rate-of-growth","text":"The rate at which the running time increases as a function of input is called rate of growth . n 4 + 2n 2 + 100n + 500 ~ n 4","title":"1.10 What is rate of growth?"},{"location":"notes/#111-commonly-used-rates-of-growth","text":"Time Complexity Name Example 1 Constant Adding an element to the front of the linked list log n Logarithmic Finding an element in a sorted array n Linear Finding an element in a unsorted array n log n Linear Logarithmic Sorting n items by divide-n-conquer - MergeSort n 2 Quadratic Shortest path between two nodes in a graph n 3 Cubic Matrix Multiplication 2 n Exponential The Towers of Hanoi problem","title":"1.11 Commonly Used Rates of Growth"},{"location":"notes/#112-types-of-analysis","text":"Algorithm can be represented with multiple expressions: one for the case where it takes less time ( best case ) and another for the case where it takes more time ( worst case ). Worst case: Defines the input for which the algorithm takes a long time (slowest time to complete). f(n) = n 2 + 500 Best case: Defines the input for which the algorithm takes the least time (fastest time to complete). f(n) = n + 100n + 500 Average case: Run the algorithm many times, using many different inputs that come from some distribution that generates these inputs, compute the total running time (by adding the individual times), and divide by the number of trails. Lower Bound <= Average Time <= Upper Bound","title":"1.12 Types of Analysis"},{"location":"notes/#113-big-o-notation","text":"Represented as f(n) = O(g(n)) , where g(n) gives the maximum rate of growth for f(n) at larger values of n. n 0 , called as threshold, is the point from which we need to consider the rate of growth for a given algorithm. Defined as O(g(n)) = {f(n): there exist positive constants c and n 0 such that 0 <= f(n) <= cg(n) for all n >= n 0 }","title":"1.13 Big-O Notation"},{"location":"notes/#115-omega-notation","text":"This notation gives the tighter lower bound of the given algorithm and we represent it as f(n) = \u03a9(g(n)) . That means, at larger values of n, the tighter lower bound of f(n) is g(n). 2n = \u03a9(n), n 3 = \u03a9(n 3 ), logn = \u03a9(logn) Defined as \u03a9(g(n)) = {f(n): there exist positive constants c and n 0 such that 0 <= cg(n) <= f(n) for all n >= n 0 }","title":"1.15 Omega-\u03a9 Notation"},{"location":"notes/#116-theta-notation","text":"This notation decides whether the upper and lower bounds of a given function (algorithm) are the same. If the upper bound (O) and the lower bound (\u03a9) give the same result, then the \u0398 notation will also have the same rate of growth. Defined as \u0398(g(n)) = {f(n): there exist positive constants c 1 , c 2 and n 0 such that 0 <= c 1 g(n) <= f(n) <= c 2 g(n) for all n >= n 0 }","title":"1.16 Theta-\u0398 Notation"},{"location":"notes/#for-analysis-best-case-worst-case-and-average-we-try-to-give-the-upper-bound-o-and-lower-bound-and-average-running-time-we-generally-focus-on-upper-bound-o-gn-is-the-asymptotic-curve-for-fn","text":"","title":"For analysis (best case, worst case and average), we try to give the upper bound (O) and lower bound (\u03a9) and average running time (\u0398). We generally focus on upper bound (O). g(n) is the asymptotic curve for f(n)."},{"location":"notes/#118-guidelines-for-asymptotic-analysis","text":"Loops : The running time of a loop is the running time of the statements inside the loop (including tests) multiplied by the number of iterations. Total time = a constant c x n = cn = O(n) Nested Loops : Total running time is the product of the sizes of all the loops. Total time = c x n x n = cn 2 = O(n 2 ) Consecutive statements : Add the time complexities of each statement. Total time = c 0 + c 1 n + c 2 n 2 = O(n 2 ) If-then-else statements : Worst-case running time: the test, plus either the then part or the else part. Total time = c 0 + c 1 * n = O(n) Logarithmic complexity : An algorithm is O(logn) if it takes a constant time to cut the problem size by a fraction (usually by 1 / 2 ). Total time = log(2 k ) = klog2 = O(logn) An example: binary search (finding a word in a dictionary of n pages) ( 1 ) Look at the center point in the dictionary ( 2 ) Is the word towards the left or right of the center? ( 3 ) Repeat the process with the left or right part of the dictionary until the word is found.","title":"1.18 Guidelines for Asymptotic Analysis"},{"location":"notes/#121-master-theorem-for-divide-and-conquer-recurrences","text":"Divide and conquer algorithms divide the problem into sub-problems, each of which is a part of the original problem, and then perform some additional work to compute the final answer. Example: for a merge sort algorithm, running time equation could be T(n) = 2T( n / 2 ) + O(n)","title":"1.21 Master Theorem for Divide and Conquer Recurrences"},{"location":"notes/#126-amortized-analysis","text":"Amortized analysis is a worst-case analysis, but for a sequence of operations rather than for individual operations. It generally applies to a method that consists of a sequence of operations, where the vast majority of the operations are cheap, but some of the operations are expensive. When one event in a sequence affects the cost of later events: One particular task may be expensive. But it may leave data structure in a state that the next few operations become easier. Example : Let us consider an array of elements from which we want to find the k th smallest element. We can solve this problem using sorting. After sorting the given array, we just need to return the k th element from it. The cost of performing the sort (assuming comparison based sorting algorithm) is O(nlogn). If we perform n such selections then the average cost of each selection is O(nlogn/n) = O(logn). This clearly indicates that sorting once is reducing the complexity of subsequent operations. Observations : While the recurrence relation looks exponential, the solution to the recurrence relation here gives a different result.","title":"1.26 Amortized Analysis"},{"location":"notes/#ch-0-organization-of-chapters","text":"","title":"CH 0: Organization of Chapters"},{"location":"notes/#three-things-required-to-build-good-understanding-of-data-structure-are","text":"How the information is arranged in the memory of the computer? Become familiar with the algorithms for manipulating the information contained in the data structure. Understand the performance characteristics of the data structure. to select a suitable data structure for a particular application.","title":"Three things required to build good understanding of data structure are:"},{"location":"notes/#definitions","text":"Recursion is the process of defining a function or calculating a number by the repeated application of an algorithm. Backtracking The solution process consists of working through a sequence of decision points in which each choice leads further along some path (example Trees or Graphs). In case of incorrect choice somewhere along the way, developer has to backtrack to a previous decision point and try a different path. Backtracking algorithms use this approach, moreover, it's a form of recursion. Linked Lists (dynamic) Number of nodes can grow and shrink on demand. Used to create trees, graphs, hashing, etc. Stacks (abstract) A container of objects that are inserted and removed according to the last-in-first-out LIFO principle. Applications: Internal creation of function parameters and local variables. Compiler's syntax check for matching braces; Support for recursion. Can act as an auxiliary data structure for other abstract data types. Queues (abstract) Similar to stacks but follow FIFO . Applications: In OS, for controlling access to shared system resources such as printers, files, communication lines, disks and tapes. Computer systems must often provide a holding area buffer for messages between two processes, two programs, or even two systems. Can act as an auxiliary data structure for other abstract data types. Trees (abstract) Data organization to make the data insertion or deletion or search faster. Applications: Library db, School's student db, employ or patient db; any db would be implemented using trees. OS file systems; Can act as an auxiliary data structure for other abstract data types. Tree variants are classified by the number of children and the way of interconnecting them. Priority Queues (abstract) Maintain a collection of prioritized elements. Application: operating systems often use a priority queue for the ready queue of processes to run on the CPU. Graphs (fundamental - non linear) Collection of nodes vertices and their connections edges . Used to model many types of relations and processes. Disjoint Set (abstract) Collection of disjoint sets partition . Example: Motorola , YouTube , and Android are all members of the Google set. Two operations: Union: merging of two sets into one! Find Query: takes an item and returns which set it belongs to. Application: Maze generation and Kruskal's algo for computing the minimum spanning tree of a graph. Sorting Arranges the elements of a list in a certain order; sometimes reduces the complexity of problem significantly. Applications: searching elements and db algorithms. Searching The process of finding an item with specified properties from a collection of items. Data organization improves the searching process. Selection Algo ( k th order statistic) Finding the minimum, maximum and median elements in a given list. Symbol Tables (dictionaries) Examples: spelling checker, data dictionary found in DBMS, symbol tables generated by loaders, assemblers, and compilers, as well as routing tables in networking components (DNS Lookup). Hashing Technique used for storing and retrieving information as fast as possible. Used in optimal search and symbol tables. The worst case complexity of hashing is O(n), but it gives O(1) on the average. String Algo Operations like auto completion (on CLI or browser) or string matching need a data structure which stores the string data efficiently. Algo Design Techniques There are different ways of classifying the algorithms like Greedy, Divide and Conquer, and Dynamic Programming. Greedy Algo ( single-minded algorithm) Process that looks for a simple, easy to implement solutions to complex, multi-step problems by deciding which next step will provide the most obvious benefit. Example: selection sort, Prim's algorithms, Kruskal's algorithms, Dijkstra algorithm, Huffman coding algorithm etc. Divide and Conquer Principles: (examples include binary search, merge sort, etc.) Divide - break the problem into several subproblems that are similar to the original problem but smaller in size. Conquer - solve the subproblems recursively. Base case - If the subproblem size is small enough then solve the subproblem directly without more recursion. Combine - the solutions to create a solution for the original problem. Dynamic Programming (optimization over plain recursion) Breaking down a complex problem into a collection of simpler subproblems. Solving each of those and storing their solutions using a memory-based data structure (array, map, etc). Complexity Classes (resources required during computation - time vs space) Classification is done based on the running time (or memory) that an algorithm takes for solving the problem. Bit-wise Hacking Applications: Kind of file format or network protocol that uses individual bits or group of bits to represent pieces of information. Setting individual pixels on the screen by directly manipulating the video memory, where every pixel's color is represented by 1 or 4 bits. Compute some kind of checksum (possibly, parity or CRC) or hash value.","title":"Definitions"},{"location":"problems/","text":"Problems CH 2: Recursion and Backtracking Problem-1: Discuss Towers of Hanoi puzzle. The Towers of Hanoi consists of three rods (or pegs or towers) and a number of disks of different sizes which can slide onto any rod. The puzzle starts with the disks on one rod in ascending order of size, the smallest at the top, thus making a conical shape. The objective of the puzzle is to move the entire stack to another rod, satisfying the following rules: Only one disk may be moved at a time. Each move consists of taking the upper disk from one of the rods and sliding it onto another rod, on top of the other disks that may already be present on that rod. No disk may be placed on top of a smaller disk. Algorithm: Move the top n-1 disks from Source to Auxiliary tower, Move the n th disk from Source to Destination tower, Move the n-1 disks from Auxiliary tower to Destination tower. Transferring the top n-1 disks from Source to Auxiliary tower can again be thought of as a fresh problem and can be solved in the same manner. def towers_of_hanoi(numberOfDisks, startPeg=1, endPeg=3): if numberOfDisks: towers_of_hanoi(numberOfDisks-1, startPeg, 6-startPeg-endPeg) print(\"Move disk %d from peg %d to peg %d\" % (numberOfDisks, startPeg, endPeg)) towers_of_hanoi(numberOfDisks-1, 6-startPeg-endPeg, endPeg) towers_of_hanoi(numberOfDisks=4) Problem-4: Generate all the strings of length n drawn from 0... k-1. Let us assume we keep current k-ary string in an array A[0.. n-1]. def range_to_list(k): result = [] for i in range(0, k): result.append(str(i)) return result def base_k_strings(n, k): if n == 0: return [] if n == 1: return range_to_list(k) return [digit+bitstring for digit in base_k_strings(1, k) for bitstring in base_k_strings(n-1, k)] print base_k_strings(4, 3) Complexity would be O(k n ). Problem-7: Path finding problem Given an nxn matrix of blocks with a source upper left block, we want to find the path from the source to the destination (the lower right block). We can only move downwards and to the left. Also a path is given by 1 and a wall is given by 0. Algorithm: If we have reached the destination point return an array containing only the position of the destination else Move in the forwards direction and check if this leads to a solution. If option a does not work, then move down. If either work, add the current position to the solution obtained at either 1 or 2. def path_finder(Matrix, position, N): # returns a list of the paths taken if position == (N-1, N-1): return [(N-1, N-1)] x, y = position if x + 1 < N and Matrix[x+1][y] == 1: a = path_finder(Matrix, (x + 1, y), N) if a != None: return [(x, y)] + a if y + 1 < N and Matrix[x][y+1] == 1: b = path_finder(Matrix, (x, y + 1), N) if b != None: return [(x, y)] + b Matrix = [[1,1,1,1,0],[0,1,0,1,0],[0,1,0,1,0],[0,1,0,0,0],[1,1,1,1,1]] print path_finder(Matrix, (0,0), 5) CH 1: Introduction Problem-28: Write a recursive function for the running time T(n) of the function given below. def function(n): count = 0 if n <= 0: return for i in range(0, n): # Outer loop executes n times for j in range(0, n): # Inner loop executes n times count = count + 1 function(n-3) # Recursive call print(count) function(20) The recursive for this call is T(n) = T(n-3) + cn 2 for some constant c > 0. Using Substraction and Conquer master theorem, we get T(n) = \u0398(n 3 ) Problem-34: Consider the following program. def Fib(n): if n == 0: return 0 elif n == 1: return 1 else: return Fib(n-1) + Fib(n-2) print(Fib(3)) The recurrence relation for the running time of this program is: T(n) = T(n-1) + T(n-2) + c . Note T(n) has two recurrence calls indicating a binary tree. Each step recursively calls the program for n reduced by 1 and 2, so the depth of the recurrence tree is O(n). The number of leaves at depth n is 2 n since this is a full binary tree, and each leaf takes at least O(1) computations for the constant factor. Running time is clearly exponential in n and it is O(2 n ). Problem-38: What is the running time of the following recursive function? def function(n): if n <= 0: return 0 for i in range(0, 3): function(n-1) function(20) T(n) = c + 3T(n-1), if n > 1 = \u0398(3 n ) Problem-49: Find the complexity of the below function. count = 0 def logarithms(n): i = 1 global count while i <= n: j = n while j > 0: j = j // 2 # this loop executes logn times count = count + 1 i = i * 2 # this loop executes logn times return count print(logarithms(10)) T(n) = O(logn * logn) = O(log 2 n)","title":"Problems"},{"location":"problems/#problems","text":"","title":"Problems"},{"location":"problems/#ch-2-recursion-and-backtracking","text":"","title":"CH 2: Recursion and Backtracking"},{"location":"problems/#problem-1-discuss-towers-of-hanoi-puzzle","text":"The Towers of Hanoi consists of three rods (or pegs or towers) and a number of disks of different sizes which can slide onto any rod. The puzzle starts with the disks on one rod in ascending order of size, the smallest at the top, thus making a conical shape. The objective of the puzzle is to move the entire stack to another rod, satisfying the following rules: Only one disk may be moved at a time. Each move consists of taking the upper disk from one of the rods and sliding it onto another rod, on top of the other disks that may already be present on that rod. No disk may be placed on top of a smaller disk. Algorithm: Move the top n-1 disks from Source to Auxiliary tower, Move the n th disk from Source to Destination tower, Move the n-1 disks from Auxiliary tower to Destination tower. Transferring the top n-1 disks from Source to Auxiliary tower can again be thought of as a fresh problem and can be solved in the same manner. def towers_of_hanoi(numberOfDisks, startPeg=1, endPeg=3): if numberOfDisks: towers_of_hanoi(numberOfDisks-1, startPeg, 6-startPeg-endPeg) print(\"Move disk %d from peg %d to peg %d\" % (numberOfDisks, startPeg, endPeg)) towers_of_hanoi(numberOfDisks-1, 6-startPeg-endPeg, endPeg) towers_of_hanoi(numberOfDisks=4)","title":"Problem-1: Discuss Towers of Hanoi puzzle."},{"location":"problems/#problem-4-generate-all-the-strings-of-length-n-drawn-from-0-k-1","text":"Let us assume we keep current k-ary string in an array A[0.. n-1]. def range_to_list(k): result = [] for i in range(0, k): result.append(str(i)) return result def base_k_strings(n, k): if n == 0: return [] if n == 1: return range_to_list(k) return [digit+bitstring for digit in base_k_strings(1, k) for bitstring in base_k_strings(n-1, k)] print base_k_strings(4, 3) Complexity would be O(k n ).","title":"Problem-4: Generate all the strings of length n drawn from 0... k-1."},{"location":"problems/#problem-7-path-finding-problem","text":"Given an nxn matrix of blocks with a source upper left block, we want to find the path from the source to the destination (the lower right block). We can only move downwards and to the left. Also a path is given by 1 and a wall is given by 0. Algorithm: If we have reached the destination point return an array containing only the position of the destination else Move in the forwards direction and check if this leads to a solution. If option a does not work, then move down. If either work, add the current position to the solution obtained at either 1 or 2. def path_finder(Matrix, position, N): # returns a list of the paths taken if position == (N-1, N-1): return [(N-1, N-1)] x, y = position if x + 1 < N and Matrix[x+1][y] == 1: a = path_finder(Matrix, (x + 1, y), N) if a != None: return [(x, y)] + a if y + 1 < N and Matrix[x][y+1] == 1: b = path_finder(Matrix, (x, y + 1), N) if b != None: return [(x, y)] + b Matrix = [[1,1,1,1,0],[0,1,0,1,0],[0,1,0,1,0],[0,1,0,0,0],[1,1,1,1,1]] print path_finder(Matrix, (0,0), 5)","title":"Problem-7: Path finding problem"},{"location":"problems/#ch-1-introduction","text":"","title":"CH 1: Introduction"},{"location":"problems/#problem-28-write-a-recursive-function-for-the-running-time-tn-of-the-function-given-below","text":"def function(n): count = 0 if n <= 0: return for i in range(0, n): # Outer loop executes n times for j in range(0, n): # Inner loop executes n times count = count + 1 function(n-3) # Recursive call print(count) function(20) The recursive for this call is T(n) = T(n-3) + cn 2 for some constant c > 0. Using Substraction and Conquer master theorem, we get T(n) = \u0398(n 3 )","title":"Problem-28: Write a recursive function for the running time T(n) of the function given below."},{"location":"problems/#problem-34-consider-the-following-program","text":"def Fib(n): if n == 0: return 0 elif n == 1: return 1 else: return Fib(n-1) + Fib(n-2) print(Fib(3)) The recurrence relation for the running time of this program is: T(n) = T(n-1) + T(n-2) + c . Note T(n) has two recurrence calls indicating a binary tree. Each step recursively calls the program for n reduced by 1 and 2, so the depth of the recurrence tree is O(n). The number of leaves at depth n is 2 n since this is a full binary tree, and each leaf takes at least O(1) computations for the constant factor. Running time is clearly exponential in n and it is O(2 n ).","title":"Problem-34: Consider the following program."},{"location":"problems/#problem-38-what-is-the-running-time-of-the-following-recursive-function","text":"def function(n): if n <= 0: return 0 for i in range(0, 3): function(n-1) function(20) T(n) = c + 3T(n-1), if n > 1 = \u0398(3 n )","title":"Problem-38: What is the running time of the following recursive function?"},{"location":"problems/#problem-49-find-the-complexity-of-the-below-function","text":"count = 0 def logarithms(n): i = 1 global count while i <= n: j = n while j > 0: j = j // 2 # this loop executes logn times count = count + 1 i = i * 2 # this loop executes logn times return count print(logarithms(10)) T(n) = O(logn * logn) = O(log 2 n)","title":"Problem-49: Find the complexity of the below function."},{"location":"review/","text":"Data Structure Basics Arrays Definition Stores data elements based on an sequential, most commonly 0 based, index. Based on Tuples from set theory. They are one of the oldest, most commonly used data structure. Can be many-dimensional to represent matrices What you need to know Optimal for indexing; bad at searching, inserting, and deleting (except at the end). Linear Arrays , or one dimensional arrays are the most basic. Are static in size, meaning that they are declared with a fixed size. Dynamic Arrays are like one dimensional arrays, but have reserved space for additional elements. If a dynamic array is full, it copies it's contents to a larger array. Two Dimensional Arrays have x and y indices like a grid or nested arrays. Big O Efficiency Indexing Linear Array: O(1), Dynamic Array: O(1) Search Linear Array: O(n), Dynamic Array: O(n) Optimized Search Linear Array: O(log n), Dynamic Array: O(log n) is Binary Search Insertion Linear Array: N/A, Dynamic Array: O(n) Linked List Definition Stores data with nodes that points to other nodes. Nodes, at its most basic it has one datum and one reference (another node). A linked list chains nodes together by pointing one node's reference towards another node. What you need to know Designed to optimize insertion and deletion, slow at indexing and searching. Doubly linked list has nodes that also reference the previous node. Circular linked list is simple linked list whose tail , the last node, references the head , the first node. Stack , commonly implemented with linked lists but can be made from arrays too. Stack are last in, first out (LIFO) data structures. Made with a linked list by having the head be the only place for insertion and removal. Queues , too can be implemented with an linked list or an array. Queues are a first in, first out (FIFO) data structure. Made with a doubly linked list that only removes from head and adds to tail. Big O Efficiency Indexing Linked Lists: O(n) Search Linked Lists: O(n) Optimized Search Linked Lists: O(n) Insertion Linked Lists: O(n) Hash Table or Hash Map Definition Stores data with key value pairs. And don't generally have an order. Hash functions accept arbitrary sized data and map it to fixed size data. The mapping is not guaranteed to be unique. This is known as hashing , which is the concept that an input and an output have a one-to-one correspondence to map information. Hash functions return a unique address in memory for that data. What you need to know Designed to optimize searching, insertion, and deletion Hash collisions are when a hash function returns the same output for two distinct inputs. All hash functions have this problem. This is often accommodated for by having the hash tables be very large. Hashes are important for associated arrays and database indexing. Big O Efficiency Indexing Hash Tables: O(1) Search Hash Tables: O(1) Insertion Hash Tables: O(1) Binary Tree Definition Is a tree like data structure where every node has at most two children. There is one left and right child node. What you need to know Designed to optimize searching and sorting. A degenerate tree is an unbalanced tree, which if entirely one-sided is an essentially a linked list. They are comparably simple to implement than other data structures. Used to make binary search trees . A binary tree that uses comparable keys to assign which direction a child is. Left child has a key smaller than it's parent node. Right child has a key greater than it's parent node. There can be no duplicate node. Because of the above it is more likely to be used as a data structure than a binary tree. Big O Efficiency Indexing Binary Search Tree: O(log n) Search Binary Search Tree: O(log n) Insertion Binary Search Tree: O(log n) Search Basics Breadth First Search Definition An algorithm that searches a tree (or graph) by searching levels of the tree first, starting at the root. It finds every node on the same level, most often moving left to right. While doing this, it tracks the children nodes of the nodes on the current level. When finishing examining a level it moves to the left most node on the next level. The bottom-right most node is evaluated last (the node that is deepest and is farthest right of it's level). What you need to know Optimal for searching a tree that is wider than it is deep. Uses a queue to store information about the tree while it traverses a tree. Because it uses a queue it is more memory intensive than depth first search . The queue uses more memory because it needs to store pointers. Big O Efficiency Search Breadth First Search: O(|E| + |V|) E is number of edges V is number of vertices Depth First Search An algorithm that searches a tree (or graph) by searching depth of the tree first, starting at the root. It traverses left down a tree until it cannot go further. Once it reaches the end of a branch it traverses back up trying the right child of nodes on that branch, and if possible left from that right children. When finished examining a branch it moves to the node right of the root then tries to go left on all it's children until it reaches the bottom. The right most node is evaluated last (the node that is right of all it's ancestors). What you need to know Optimal for searching a tree that is deeper than it is wide. Uses a stack to push nodes onto. Because a stack is LIFO it does not need to keep track of the nodes pointers and is therefore less memory intensive than breadth first search . Once it cannot go further left it begins evaluating the stack. Big O Efficiency Search Depth First Search: O(|E| + |V|) E is number of edges. V is number of vertices. Breadth First Search Vs Depth First Search The simple to this question is that it depends on the size and shape of the tree. For wide, shallow trees use Breadth First Search For deep, narrow trees use Depth First Search Nuances: Because BFS uses queues to store information about the nodes and it's children, it could use more memory than is available on your computer. (But you probably won't have to worry about this.) If using a DFS on a tree that is very deep you might go unnecessarily deep in the search. Breadth First Search tends to be a looping algorithm. Depth First Search tends to be a recursive algorithm. Breadth-first search is guaranteed to find a shortest possible path through a graph. Depth-first search is not (and usually does not). DFS and BFS have no defined order in which the elements are searched. ( In the specific case of searching a tree for a specific node, there is only one possible path, so both will return the same result, but search algorithms are very rarely used this way; typically, they operate on arbitrary graphs. Additionally worth knowing is that most practically-used searching and pathfinding algorithms (e.g. Dijkstra's algorithm, A) are specializations of breadth-first search. ) Worst case space complexity: Data Structures: Array: O(n) Linked List: O(n) Hash Table: O(n) BST: O(n) Sorting: Quick Sort: O(log(n)) Merge Sort: O(n) Bubble Sort: O(1) Efficient Sorting Basics Merge Sort Definition A comparision based sorting algorithm Divides entire data set into groups of at most two. Compares each number one at a time, moving the smallest number to left of the pair. Once all pairs sorted then, it compares left most elements of the two left most pairs creating a sorted group of four with the smallest numbers on the left and the largest ones on the right. This process is repeated until there is only one set. What you need to know This is one of the most basic sorting algorithms. Know that it divides all the data into as small possible sets then compares them. Big O Efficiency Best Case Sort: Merge Sort O(n log n) Average Case Sort: Merge Sort O(n log n) Worst Case Sort: Merge Sort O(n log n) Quick Sort Definition A comparision based sorting algorithm Divides entire dataset in half by selecting the middle element and putting all smaller elements to the left of the average. It repeats this process on the left side until it is comparing only two elements at which point the left side is sorted. When the left side is finished sorting it performs the same operation on the right side. Computer architecture favours the quicksort process. (Edit: Quicksort picks a random element from the dataset as the pivot element, then sorts all elements smaller before that and all greater than the pivot after it. Then quicksort is executed on the part left of the pivot and right of it .) What you need to know While it has the same Big O as (or worse in some cases) many other sorting algorithms it is often faster in practice than merge sort. Know that it halves the data set by the average continuously until all the information is sorted. Big O Efficiency Best Case Sort: Quick Sort: O(n) Average Case Sort: Quick Sort: O(n log n) Worst Case Sort: Quick Sort: O(n^2) Bubble Sort Definition A comparision based sorting algorithm It iterates left to right comparing every couplet, moving the smaller element to the left. It repeats this process until it no longer moves an element to the left. What you need to know While it is very simple to implement, it is the least efficient of these three sorting methods. Know that it moves one space to the right comparing two elements at a time and moving the smaller on to left. Big O Efficiency Best Case Sort: Bubble Sort: O(n) Average Case Sort: Bubble Sort: O(n^2) Worst Case Sort: Bubble Sort: O(n^2) Merge Sort vs Quick Sort Quick Sort is likely faster in practice. Merge Sort divides the set into the smaller possible groups immediately then reconstructs the incrementally as it sorts the groupings. Quick Sort continually divides the set by the average, until the set is recursively sorted. Basic Types of Algorithms Recursive Algorithms Definition An algorithm that calls itself in it's definition. Recursive case is a conditional statement that is used to trigger the recursion. Base case is a conditional statement that is used to break the recursion. What you need to know Stack level too deep and stack overflow If you have seen either of these from a recursive algorithm, you messed up. It means that your base case was never triggered because it was faulty or the problem was so massive you ran out of stack before reaching it. Knowing weather or not you will reach a base case is integral to correctly using recursion. Often used in Depth First Search . Iterative Algorithms Definition An algorithm that is called repeatedly but for a finite number of times, each time being a single iteration. Often used to move incrementally through a data set. What you need to know Generally you will see iteration as loops: for, while and until statements. Think of iteration as moving one at a time through a set. Often used to move through an array. Recursion vs Iteration The differences between recursion and iteration can be confusing to distinguish since both can be used to implement the other. But know that, Recursion is, usually, more expressive and easier to implement. Iteration uses less memory. ( that's not true when the compiler or interpreter supports tail-call elimination, or can otherwise optimize recursive functions. ) Functional languages tend to use recursion. (i.e. Haskell) Imperative languages tend to use iteration. (i.e. Ruby) Greedy Algorithm Definition An algorithm that, while executing, selects only the information that meets a certain criteria. The general five components: A candidate set, from which a solution is created. A selection function, which chooses the best candidate to be added to the solution. A feasibility function, that is used to determine if a candidate can be used to contribute to a solution. An objective function, which assigns a valve to a solution, or a partial solution. A solution function, which indicate when we have discovered a complete solution. What you need to know Used to find the optimal solution to a given problem. Generally used on sets of data where only a small proportion of the information evaluated meets the desired result. Often a greedy algorithm can help reduce the Big O of an algorithm. Pseudo Code of a Greedy Algorithm to find largest difference of any two numbers in an array greedy algorithm (array) var largest difference = 0 var new difference = find next difference (array[n], array[n+1]) largest difference = new difference if new difference is > largest difference repeat above two steps until all the differences have been found return largest difference This algorithm never needed to compare all the differences to one another, saving it an entire iteration. Stream I/O Definition A stream is an abstract representation of sequence of bytes. It is usually buffered to prevent representing the entire sequence of bytes in memory at one time. Generally one-way and forward-only Has significant advantages over a standard array of bytes What you need to know Can be stored: In memory Transferred over a network On disk In a database Advantages over byte arrays Efficient use of memory Smaller memory footprint Uses Transferring files between persistence locations Compression / Decompression Encryption / Decryption Throttling Can be chained Some are one-way (NetworkStream, CipherStream, etc), others can seek in either direction. Summary S.No. Name Indexing Search Insertion Optimized Search 1 Linear Array O(1) O(n) N/A O(log n) 2 Dynamic Array O(1) O(n) o(n) O(log n) 3 Linked List O(n) O(n) O(1) O(n) 4 Hash Table O(1) O(1) O(1) 5 Binary Tree O(log n) O(log n) O(log n)","title":"Quick Review"},{"location":"review/#data-structure-basics","text":"","title":"Data Structure Basics"},{"location":"review/#arrays","text":"","title":"Arrays"},{"location":"review/#definition","text":"Stores data elements based on an sequential, most commonly 0 based, index. Based on Tuples from set theory. They are one of the oldest, most commonly used data structure. Can be many-dimensional to represent matrices","title":"Definition"},{"location":"review/#what-you-need-to-know","text":"Optimal for indexing; bad at searching, inserting, and deleting (except at the end). Linear Arrays , or one dimensional arrays are the most basic. Are static in size, meaning that they are declared with a fixed size. Dynamic Arrays are like one dimensional arrays, but have reserved space for additional elements. If a dynamic array is full, it copies it's contents to a larger array. Two Dimensional Arrays have x and y indices like a grid or nested arrays.","title":"What you need to know"},{"location":"review/#big-o-efficiency","text":"Indexing Linear Array: O(1), Dynamic Array: O(1) Search Linear Array: O(n), Dynamic Array: O(n) Optimized Search Linear Array: O(log n), Dynamic Array: O(log n) is Binary Search Insertion Linear Array: N/A, Dynamic Array: O(n)","title":"Big O Efficiency"},{"location":"review/#linked-list","text":"","title":"Linked List"},{"location":"review/#definition_1","text":"Stores data with nodes that points to other nodes. Nodes, at its most basic it has one datum and one reference (another node). A linked list chains nodes together by pointing one node's reference towards another node.","title":"Definition"},{"location":"review/#what-you-need-to-know_1","text":"Designed to optimize insertion and deletion, slow at indexing and searching. Doubly linked list has nodes that also reference the previous node. Circular linked list is simple linked list whose tail , the last node, references the head , the first node. Stack , commonly implemented with linked lists but can be made from arrays too. Stack are last in, first out (LIFO) data structures. Made with a linked list by having the head be the only place for insertion and removal. Queues , too can be implemented with an linked list or an array. Queues are a first in, first out (FIFO) data structure. Made with a doubly linked list that only removes from head and adds to tail.","title":"What you need to know"},{"location":"review/#big-o-efficiency_1","text":"Indexing Linked Lists: O(n) Search Linked Lists: O(n) Optimized Search Linked Lists: O(n) Insertion Linked Lists: O(n)","title":"Big O Efficiency"},{"location":"review/#hash-table-or-hash-map","text":"","title":"Hash Table or Hash Map"},{"location":"review/#definition_2","text":"Stores data with key value pairs. And don't generally have an order. Hash functions accept arbitrary sized data and map it to fixed size data. The mapping is not guaranteed to be unique. This is known as hashing , which is the concept that an input and an output have a one-to-one correspondence to map information. Hash functions return a unique address in memory for that data.","title":"Definition"},{"location":"review/#what-you-need-to-know_2","text":"Designed to optimize searching, insertion, and deletion Hash collisions are when a hash function returns the same output for two distinct inputs. All hash functions have this problem. This is often accommodated for by having the hash tables be very large. Hashes are important for associated arrays and database indexing.","title":"What you need to know"},{"location":"review/#big-o-efficiency_2","text":"Indexing Hash Tables: O(1) Search Hash Tables: O(1) Insertion Hash Tables: O(1)","title":"Big O Efficiency"},{"location":"review/#binary-tree","text":"","title":"Binary Tree"},{"location":"review/#definition_3","text":"Is a tree like data structure where every node has at most two children. There is one left and right child node.","title":"Definition"},{"location":"review/#what-you-need-to-know_3","text":"Designed to optimize searching and sorting. A degenerate tree is an unbalanced tree, which if entirely one-sided is an essentially a linked list. They are comparably simple to implement than other data structures. Used to make binary search trees . A binary tree that uses comparable keys to assign which direction a child is. Left child has a key smaller than it's parent node. Right child has a key greater than it's parent node. There can be no duplicate node. Because of the above it is more likely to be used as a data structure than a binary tree.","title":"What you need to know"},{"location":"review/#big-o-efficiency_3","text":"Indexing Binary Search Tree: O(log n) Search Binary Search Tree: O(log n) Insertion Binary Search Tree: O(log n)","title":"Big O Efficiency"},{"location":"review/#search-basics","text":"","title":"Search Basics"},{"location":"review/#breadth-first-search","text":"","title":"Breadth First Search"},{"location":"review/#definition_4","text":"An algorithm that searches a tree (or graph) by searching levels of the tree first, starting at the root. It finds every node on the same level, most often moving left to right. While doing this, it tracks the children nodes of the nodes on the current level. When finishing examining a level it moves to the left most node on the next level. The bottom-right most node is evaluated last (the node that is deepest and is farthest right of it's level).","title":"Definition"},{"location":"review/#what-you-need-to-know_4","text":"Optimal for searching a tree that is wider than it is deep. Uses a queue to store information about the tree while it traverses a tree. Because it uses a queue it is more memory intensive than depth first search . The queue uses more memory because it needs to store pointers.","title":"What you need to know"},{"location":"review/#big-o-efficiency_4","text":"Search Breadth First Search: O(|E| + |V|) E is number of edges V is number of vertices","title":"Big O Efficiency"},{"location":"review/#depth-first-search","text":"An algorithm that searches a tree (or graph) by searching depth of the tree first, starting at the root. It traverses left down a tree until it cannot go further. Once it reaches the end of a branch it traverses back up trying the right child of nodes on that branch, and if possible left from that right children. When finished examining a branch it moves to the node right of the root then tries to go left on all it's children until it reaches the bottom. The right most node is evaluated last (the node that is right of all it's ancestors).","title":"Depth First Search"},{"location":"review/#what-you-need-to-know_5","text":"Optimal for searching a tree that is deeper than it is wide. Uses a stack to push nodes onto. Because a stack is LIFO it does not need to keep track of the nodes pointers and is therefore less memory intensive than breadth first search . Once it cannot go further left it begins evaluating the stack.","title":"What you need to know"},{"location":"review/#big-o-efficiency_5","text":"Search Depth First Search: O(|E| + |V|) E is number of edges. V is number of vertices.","title":"Big O Efficiency"},{"location":"review/#breadth-first-search-vs-depth-first-search","text":"The simple to this question is that it depends on the size and shape of the tree. For wide, shallow trees use Breadth First Search For deep, narrow trees use Depth First Search","title":"Breadth First Search Vs Depth First Search"},{"location":"review/#nuances","text":"Because BFS uses queues to store information about the nodes and it's children, it could use more memory than is available on your computer. (But you probably won't have to worry about this.) If using a DFS on a tree that is very deep you might go unnecessarily deep in the search. Breadth First Search tends to be a looping algorithm. Depth First Search tends to be a recursive algorithm. Breadth-first search is guaranteed to find a shortest possible path through a graph. Depth-first search is not (and usually does not). DFS and BFS have no defined order in which the elements are searched. ( In the specific case of searching a tree for a specific node, there is only one possible path, so both will return the same result, but search algorithms are very rarely used this way; typically, they operate on arbitrary graphs. Additionally worth knowing is that most practically-used searching and pathfinding algorithms (e.g. Dijkstra's algorithm, A) are specializations of breadth-first search. )","title":"Nuances:"},{"location":"review/#worst-case-space-complexity","text":"","title":"Worst case space complexity:"},{"location":"review/#data-structures","text":"Array: O(n) Linked List: O(n) Hash Table: O(n) BST: O(n)","title":"Data Structures:"},{"location":"review/#sorting","text":"Quick Sort: O(log(n)) Merge Sort: O(n) Bubble Sort: O(1)","title":"Sorting:"},{"location":"review/#efficient-sorting-basics","text":"","title":"Efficient Sorting Basics"},{"location":"review/#merge-sort","text":"","title":"Merge Sort"},{"location":"review/#definition_5","text":"A comparision based sorting algorithm Divides entire data set into groups of at most two. Compares each number one at a time, moving the smallest number to left of the pair. Once all pairs sorted then, it compares left most elements of the two left most pairs creating a sorted group of four with the smallest numbers on the left and the largest ones on the right. This process is repeated until there is only one set.","title":"Definition"},{"location":"review/#what-you-need-to-know_6","text":"This is one of the most basic sorting algorithms. Know that it divides all the data into as small possible sets then compares them.","title":"What you need to know"},{"location":"review/#big-o-efficiency_6","text":"Best Case Sort: Merge Sort O(n log n) Average Case Sort: Merge Sort O(n log n) Worst Case Sort: Merge Sort O(n log n)","title":"Big O Efficiency"},{"location":"review/#quick-sort","text":"","title":"Quick Sort"},{"location":"review/#definition_6","text":"A comparision based sorting algorithm Divides entire dataset in half by selecting the middle element and putting all smaller elements to the left of the average. It repeats this process on the left side until it is comparing only two elements at which point the left side is sorted. When the left side is finished sorting it performs the same operation on the right side. Computer architecture favours the quicksort process. (Edit: Quicksort picks a random element from the dataset as the pivot element, then sorts all elements smaller before that and all greater than the pivot after it. Then quicksort is executed on the part left of the pivot and right of it .)","title":"Definition"},{"location":"review/#what-you-need-to-know_7","text":"While it has the same Big O as (or worse in some cases) many other sorting algorithms it is often faster in practice than merge sort. Know that it halves the data set by the average continuously until all the information is sorted.","title":"What you need to know"},{"location":"review/#big-o-efficiency_7","text":"Best Case Sort: Quick Sort: O(n) Average Case Sort: Quick Sort: O(n log n) Worst Case Sort: Quick Sort: O(n^2)","title":"Big O Efficiency"},{"location":"review/#bubble-sort","text":"","title":"Bubble Sort"},{"location":"review/#definition_7","text":"A comparision based sorting algorithm It iterates left to right comparing every couplet, moving the smaller element to the left. It repeats this process until it no longer moves an element to the left.","title":"Definition"},{"location":"review/#what-you-need-to-know_8","text":"While it is very simple to implement, it is the least efficient of these three sorting methods. Know that it moves one space to the right comparing two elements at a time and moving the smaller on to left.","title":"What you need to know"},{"location":"review/#big-o-efficiency_8","text":"Best Case Sort: Bubble Sort: O(n) Average Case Sort: Bubble Sort: O(n^2) Worst Case Sort: Bubble Sort: O(n^2)","title":"Big O Efficiency"},{"location":"review/#merge-sort-vs-quick-sort","text":"Quick Sort is likely faster in practice. Merge Sort divides the set into the smaller possible groups immediately then reconstructs the incrementally as it sorts the groupings. Quick Sort continually divides the set by the average, until the set is recursively sorted.","title":"Merge Sort vs Quick Sort"},{"location":"review/#basic-types-of-algorithms","text":"","title":"Basic Types of Algorithms"},{"location":"review/#recursive-algorithms","text":"","title":"Recursive Algorithms"},{"location":"review/#definition_8","text":"An algorithm that calls itself in it's definition. Recursive case is a conditional statement that is used to trigger the recursion. Base case is a conditional statement that is used to break the recursion.","title":"Definition"},{"location":"review/#what-you-need-to-know_9","text":"Stack level too deep and stack overflow If you have seen either of these from a recursive algorithm, you messed up. It means that your base case was never triggered because it was faulty or the problem was so massive you ran out of stack before reaching it. Knowing weather or not you will reach a base case is integral to correctly using recursion. Often used in Depth First Search .","title":"What you need to know"},{"location":"review/#iterative-algorithms","text":"","title":"Iterative Algorithms"},{"location":"review/#definition_9","text":"An algorithm that is called repeatedly but for a finite number of times, each time being a single iteration. Often used to move incrementally through a data set.","title":"Definition"},{"location":"review/#what-you-need-to-know_10","text":"Generally you will see iteration as loops: for, while and until statements. Think of iteration as moving one at a time through a set. Often used to move through an array.","title":"What you need to know"},{"location":"review/#recursion-vs-iteration","text":"The differences between recursion and iteration can be confusing to distinguish since both can be used to implement the other. But know that, Recursion is, usually, more expressive and easier to implement. Iteration uses less memory. ( that's not true when the compiler or interpreter supports tail-call elimination, or can otherwise optimize recursive functions. ) Functional languages tend to use recursion. (i.e. Haskell) Imperative languages tend to use iteration. (i.e. Ruby)","title":"Recursion vs Iteration"},{"location":"review/#greedy-algorithm","text":"","title":"Greedy Algorithm"},{"location":"review/#definition_10","text":"An algorithm that, while executing, selects only the information that meets a certain criteria. The general five components: A candidate set, from which a solution is created. A selection function, which chooses the best candidate to be added to the solution. A feasibility function, that is used to determine if a candidate can be used to contribute to a solution. An objective function, which assigns a valve to a solution, or a partial solution. A solution function, which indicate when we have discovered a complete solution.","title":"Definition"},{"location":"review/#what-you-need-to-know_11","text":"Used to find the optimal solution to a given problem. Generally used on sets of data where only a small proportion of the information evaluated meets the desired result. Often a greedy algorithm can help reduce the Big O of an algorithm.","title":"What you need to know"},{"location":"review/#pseudo-code-of-a-greedy-algorithm-to-find-largest-difference-of-any-two-numbers-in-an-array","text":"greedy algorithm (array) var largest difference = 0 var new difference = find next difference (array[n], array[n+1]) largest difference = new difference if new difference is > largest difference repeat above two steps until all the differences have been found return largest difference This algorithm never needed to compare all the differences to one another, saving it an entire iteration.","title":"Pseudo Code of a Greedy Algorithm to find largest difference of any two numbers in an array"},{"location":"review/#stream-io","text":"","title":"Stream I/O"},{"location":"review/#definition_11","text":"A stream is an abstract representation of sequence of bytes. It is usually buffered to prevent representing the entire sequence of bytes in memory at one time. Generally one-way and forward-only Has significant advantages over a standard array of bytes","title":"Definition"},{"location":"review/#what-you-need-to-know_12","text":"Can be stored: In memory Transferred over a network On disk In a database Advantages over byte arrays Efficient use of memory Smaller memory footprint Uses Transferring files between persistence locations Compression / Decompression Encryption / Decryption Throttling Can be chained Some are one-way (NetworkStream, CipherStream, etc), others can seek in either direction.","title":"What you need to know"},{"location":"review/#summary","text":"S.No. Name Indexing Search Insertion Optimized Search 1 Linear Array O(1) O(n) N/A O(log n) 2 Dynamic Array O(1) O(n) o(n) O(log n) 3 Linked List O(n) O(n) O(1) O(n) 4 Hash Table O(1) O(1) O(1) 5 Binary Tree O(log n) O(log n) O(log n)","title":"Summary"}]}